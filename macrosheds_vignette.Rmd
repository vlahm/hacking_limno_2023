---
title: 'Macroscale watershed science'
subtitle: 'Hacking Limnology 2023'
output:
    html_document:
        theme: 'lumen'
        toc: yes
        toc_float:
            collapsed: true
        toc_depth: 3
---

```{r setup1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = ' ')
```

---

# Before the workshop

Please install the following packages:
```{r install, eval=FALSE}

#macrosheds
devtools::install_github('https://github.com/MacroSHEDS/macrosheds.git')

#tidyverse (dplyr, stringr, readr, etc.)
install.packages('tidyverse')
```

Then download the MacroSheds data we will use
```{r dl, eval=FALSE}

library(macrosheds)

# this is where files will be downloaded. feel free to change it.
project_root <- '~/macrosheds_workshop'

# datasets we will use
ms_download_ws_attr(project_root, dataset = 'summaries')
ms_download_core_data(project_root, domains = c('niwot', 'plum', 'east_river', 'santee'))

```

These packages are optional (only used here and there, and the overall sequence doesn't depend on them)
```{r install optional, eval=FALSE}

install.packages('xts')         # time series representation
install.packages('dygraphs')    # interactive plots
install.packages('factoextra')  # ordination, etc.
install.packages('mapview')     # interactive maps
install.packages('data.table')  # efficient computation
install.packages('whitebox')    # geospatial processing
whitebox::install_whitebox() #additional whitebox binaries. if this fails, use the next line
# whitebox::install_whitebox(platform = 'linux_musl')
```

---

# MacroSheds

_*long-term watershed ecosystem studies, harmonized*_

## Goals
 + enable calculation of input/output solute fluxes from diverse watersheds
 + lower barriers to accessing diverse watershed data through data harmonization and visualization
 + investigate:
   + variation in magnitude, timing, form of N exports
   + broad effects of watershed acidification
   + variation in mineral weathering
   + watershed sensitivity to climate change
 
## Components
 + [macrosheds.org](https://macrosheds.org)
 + [The dataset](https://portal.edirepository.org/nis/mapbrowse?scope=edi&identifier=1262)
 + [R package](https://github.com/MacroSHEDS/macrosheds)
 + [All project code](https://github.com/MacroSHEDS)
 
## Terms
 + site: an individual gauging station or stream sampling location and its watershed
 + domain: one or more sites under common management
 + network: one or more domains under common funding/leadership
 + flux: solute mass normalized to watershed area, over time (kg/ha/d)
 
# Related resources

 + [GEMStat](https://gemstat.org/) - global water quality database
 + [GLORICH](https://www.geo.uni-hamburg.de/en/geologie/forschung/aquatische-geochemie/glorich.html) - global river chemistry database
 + [Water Quality Portal (WQP)](https://www.waterqualitydata.us/) - USA water quality database
 + [Global River Water Quality Archive](https://essd.copernicus.org/articles/13/5483/2021/) - subset of: GEMStat + GLORICH + WQP + CESI (Canada) + Waterbase (Europe)
 + CAMELS - Catchment Attributes and MEteorology for Large-sample Studies
   + [USA](https://gdex.ucar.edu/dataset/camels.html)
   + [Australia](https://www.webofscience.com/wos/woscc/full-record/WOS:000683780500004?SID=USW2EC0D4DYFZoLYt17MtaFngvTOg)
   + [Brazil](https://www.webofscience.com/wos/woscc/full-record/WOS:000569420300002?SID=USW2EC0D4DYFZoLYt17MtaFngvTOg)
   + [Chile](https://www.webofscience.com/wos/woscc/full-record/WOS:000449995800002?SID=USW2EC0D4DYFZoLYt17MtaFngvTOg)
   + [Great Britain](https://www.webofscience.com/wos/woscc/full-record/WOS:000580860000002?SID=USW2EC0D4DYFZoLYt17MtaFngvTOg)
 + [Caravan](https://zenodo.org/record/7944025) - subset of: CAMELS + HYSETS (N. Am.) + LamaH-CE (central Europe)
+ [CAMELS-Chem](https://hess.copernicus.org/preprints/hess-2022-81/) - chemistry for CAMELS-US

# Part 1: Setup and overview

```{r setup2}

suppressPackageStartupMessages(library(tidyverse))
library(macrosheds)

project_root <- '~/macrosheds_workshop'
```

## Retrieval of basic site data, watershed summary attributes

```{r retrieve1, cache=TRUE}

ms_sites <- ms_load_sites() %>% 
    filter(site_type == 'stream_gauge') #some precip gauges have the same names as stream gauges!

?ms_download_ws_attr
# ms_download_ws_attr(project_root, dataset = 'summaries')

?ms_load_product
ws_smry <- ms_load_product(project_root, prodname = 'ws_attr_summaries')
```

 + watershed summary attribute definitions: [https://portal.edirepository.org/nis/metadataviewer?packageid=edi.1262.1] -> Data Entities -> `ws_attr_summaries.csv`
 + two-letter variable prefixes: `variable_category_codes_ws_attr.csv`, `variable_data_source_codes_ws_attr.csv`

## Principal component analysis of watershed summary attributes

```{r pca}

suppressPackageStartupMessages(library(factoextra))

pca_data <- ws_smry %>% 
    select(where( ~sum(is.na(.)) / length(.) < 0.3)) %>% #drop columns with >= 30% missing values
    drop_na()                                            #drop rows with any missing values

domains <- pca_data$domain

pca_data <- pca_data %>% 
    select(-site_code, -domain, -network) %>% 
    select(where(~sd(., na.rm = TRUE) != 0)) %>%         #drop columns with no variance
    as.matrix()

smry_categories <- substr(colnames(pca_data), 1, 1)
category_map <- c('c' = 'climate', 'h' = 'hydrology', 'l' = 'landcover',
                  'p' = 'parentmat', 't' = 'terrain', 'v' = 'vegetation',
                  'w' = 'ws area')
smry_categories <- factor(category_map[match(smry_categories, names(category_map))])

pca <- prcomp(pca_data, center = TRUE, scale. = TRUE)

fviz_eig(pca)

fviz_pca_biplot(pca, geom.var = 'arrow', geom.ind = 'point', title = '',
                col.var = smry_categories, palette = 'Dark2')

fviz_pca_biplot(pca, geom.var = '', geom.ind = 'point', title = '',
                col.ind = as.factor(domains))
```

## Identify domains at elevational extremes (2 of each)

```{r identify target domains}

elev_extreme_domains <- ws_smry %>% 
    filter(! is.na(te_elev_mean)) %>% #no watersheds delineated for McMurdo LTER
    group_by(domain) %>% 
    summarize(domain_mean_elev = mean(te_elev_mean)) %>% 
    ungroup() %>% 
    arrange(desc(domain_mean_elev)) %>% 
    slice(c(1:2, (n() - 1):n())) %>% #2 highest and 2 lowest domains, by average watershed elevation
    print() %>%
    pull(domain)
```

## Load site-variable catalog; filter by domain

```{r filter1}

?ms_load_variables
sitevars <- ms_load_variables('timeseries_by_site')

sitevars <- filter(sitevars,
                   domain %in% elev_extreme_domains,
                   chem_category == 'stream_conc')

length(unique(sitevars$site_code)) # n sites
```

## What variables are shared by at least one site from each of these 4 domains?

```{r shared variables}

get_shared_domain_vars <- function(df){

    df %>% 
        group_split(domain) %>% 
        map(~ pluck(.x, 'variable_code')) %>% 
        reduce(intersect)
}

get_shared_domain_vars(sitevars)
```

## Again, but filter for variables recorded for >= 2 years, at >= average rate of 36 obs/year

```{r shared variables filt}

sitevars <- sitevars %>% 
    mutate(ndays = difftime(last_record_utc, first_record_utc, unit = 'days')) %>% 
    filter(ndays >= 2 * 365.25,
           mean_obs_per_day * 365.25 >= 36)

get_shared_domain_vars(sitevars)
```

## Retrieve core time-series data

```{r retrieve2, cache=TRUE}

?ms_download_core_data
# ms_download_core_data(project_root, domains = elev_extreme_domains)

?ms_load_product
(doc <- ms_load_product(
    project_root,
    prodname = 'stream_chemistry',
    filter_vars = 'DOC',
    domain = elev_extreme_domains,
    warn = FALSE
))

unique(doc$var)
table(doc$ms_status)
table(doc$ms_interp)
```

 + time-series column definitions: [https://portal.edirepository.org/nis/metadataviewer?packageid=edi.1262.1] -> Data Entities -> e.g. `timeseries_boulder.csv`
 + two-letter variable_code prefixes (ts only)
   + G = grab sample
   + I = installed instrument
   + S = sensor
   + N = non-sensor
 + variable units and full names are included in `sitevars`

# Part 2: Which variables best predict DOC at high and low elevation sites?

## remove questionable observations; plot time series

```{r ms_status}

doc_wide <- doc %>% 
    filter(ms_status == 0) %>% #remove ms_status == 1 (questionable)
    select(datetime, site_code, val) %>% 
    left_join(select(ms_sites, site_code, domain), #join column of domain names
              by = 'site_code') %>% 
    filter(! is.na(domain)) %>%  #some site data is missing in MS version 1 (whoops)
    pivot_wider(names_from = c(domain, site_code),
                values_from = val,
                names_sep = '__') %>% #column names are of the form <domain>__<site_code>
    arrange(datetime) %>% #make sure it's all sorted chronologically
    complete(datetime = seq(first(datetime), last(datetime), by = 'day')) #explicate missing observations

suppressPackageStartupMessages(library(xts))
library(dygraphs)

dmn_colors <- factor(str_split_fixed(colnames(doc_wide)[-1], '__', n = 2)[, 1])
levels(dmn_colors) <- hcl.colors(length(elev_extreme_domains))

dg <- dygraph(xts(x = doc_wide[, -1], order.by = doc_wide$datetime)) %>% 
    dyRangeSelector()
for(i in 2:ncol(doc_wide)){
    dg <- dySeries(dg, colnames(doc_wide)[i], color = as.character(dmn_colors[i - 1]))
}
dg
```

## Which watershed attributes predict temporal variation in DOC concentration

```{r regression}
#compute CV for each site
#join ws attrs
#glmnet
```

## Convert nitrate-N concentration to daily flux (basic mode)

```{r flux conversion}
NO3_N_conc <- ms_load_product(
    project_root,
    prodname = 'stream_chemistry',
    filter_vars = 'NO3_N',
    domain = 'niwot',
    warn = FALSE
)

Q <- ms_load_product(
    project_root,
    prodname = 'discharge',
    domain = 'niwot',
    warn = FALSE
)

NO3_N_conc <- semi_join(NO3_N_conc, Q, by = 'site_code')

#before
filter(NO3_N_conc, datetime == as.POSIXct('1985-05-09 00:00:00', tz = 'UTC'), site_code == 'ALBION')

NO3_N_flux <- suppressPackageStartupMessages({
    ms_calc_flux(NO3_N_conc, Q, q_type = 'discharge')
})

#after
filter(NO3_N_flux, datetime == as.POSIXct('1985-05-09 00:00:00', tz = 'UTC'), site_code == 'ALBION')
```

## Compute annual nitrate-N load (still in development)

```{r load calc, eval=FALSE}

# this function will be officially included in the macrosheds package when we
# release version 2 of the MacroSheds dataset, which will include monthly and
# yearly load estimates based on Gubbins et al. in prep. For now, you can call it
# with `:::`, which accesses "internal" package functions.
NO3_N_load <- macrosheds:::ms_calc_flux_rsfme(
    NO3_N_conc, Q,
    method = 'beale',
    aggregation = 'annual'
)
```

## WRTDS flux via EGRET package

```{r wrtds, eval=FALSE}

ms_run_egret(stream_chemistry = filter(NO3_N_conc, site_code == 'ALBION'),
             discharge = filter(Q, site_code == 'ALBION'))
```

## (Un)scale flux by watershed area

```{r scale by watershed area}

(kg_ha_d <- slice(NO3_N_flux, 1:3))
(kg_d <- ms_undo_scale_flux_by_area(kg_ha_d))
(kg_ha_d <- ms_scale_flux_by_area(kg_d))
#NOTE: not every function set up to perform error computations yet
```

# Part 3: Useful tools

## Automated citation/acknowledgement for any subset of the MacroSheds dataset

```{r attribution, cache=TRUE}

?ms_generate_attribution

attrib <- ms_generate_attribution(doc, chem_source = 'stream')

ls.str(attrib)
attrib$intellectual_rights_notifications
attrib$intellectual_rights_explanations
t(attrib$full_details_timeseries[1, ])

attrib <- ms_generate_attribution(doc, chem_source = 'stream', write_to_dir = '~')
read_file('~/macrosheds_attribution_information/ms_bibliography.bib') %>% 
    substr(1, 1092) %>% 
    cat()
```

## Watershed delineation

(interactive, so output not included in the HTML version of this document)

*can work where StreamStats fails, especially in small watersheds*

```{r watershed delineation, eval=FALSE}

whitebox::install_whitebox() #if this fails, use the next line
# whitebox::install_whitebox(platform = 'linux_musl')

tmp <- tempdir()

out <- ms_delineate_watershed(
    lat = 44.21013,
    long = -122.2571,
    crs = 4326,
    write_dir = tmp,
    write_name = 'example_site',
)

str(out) #specifications of your successful delineation
```

## Load spatial data

(watershed boundaries, precip gauge locations, stream gauge locations)

```{r spatial data}

suppressPackageStartupMessages(library(mapview))

ws_bound <- ms_load_spatial_product(project_root, 'ws_boundary', domain = 'boulder')
prcp_gauges <- ms_load_spatial_product(project_root, 'stream_gauge_locations', domain = 'boulder')
strm_gauges <- ms_load_spatial_product(project_root, 'precip_gauge_locations', domain = 'boulder')

mapview(ws_bound) + mapview(prcp_gauges) + mapview(strm_gauges, col.regions = rainbow(n = 3))
```

## More vignettes

Vignettes will only load if you installed `macrosheds` with `build_vignettes=TRUE`, but they're also hosted as markdown files [here](https://github.com/MacroSHEDS/macrosheds/tree/master/vignettes). These provide tutorials on data retrieval, flux calculation, watershed delineation, and precip interpolation.

```{r more vignettes, eval=FALSE}
vignette(package = 'macrosheds')
vignette('ms_watershed_delineation', package = 'macrosheds')
```

```{r other tools}

?ms_synchronize_timestep  # upsample or downsample macrosheds data
?ms_calc_watershed_precip # spatial interpolation of precip gauge data
?ms_separate_baseflow     # baseflow/stormflow separation via hydrostats
```

# Part 4: linking to the CAMELS datasets

## Download CAMELS-US (partly)

```{r camels}

camels_dir <- file.path(project_root, 'camels')
dir.create(camels_dir)

attr_categories <- c('clim', 'geol', 'soil', 'topo', 'vege', 'hydro')

#watershed attributes
for(x in attr_categories){
    download.file(paste0('https://gdex.ucar.edu/dataset/camels/file/camels_', x, '.txt'),
                  destfile = paste0(camels_dir, '/', x, '.txt'))
}

# #Daymet forcings
# download.file('https://gdex.ucar.edu/dataset/camels/file/basin_timeseries_v1p2_modelOutput_daymet.zip',
#               destfile = file.path(camels_dir, 'daymet.zip'))
# 
# #observed flow
# download.file('https://gdex.ucar.edu/dataset/camels/file/basin_timeseries_v1p2_metForcing_obsFlow.zip',
#               destfile = file.path(camels_dir, 'flow.zip'))
```

```{r camels view}

# library(plotly)

view_camels_attr <- function(path_to_camels){

    char_cols <- c('dom_land_cover', 'geol_1st_class', 'geol_2nd_class',
                   'high_prec_timing', 'low_prec_timing')
    
    vege <- read_csv2(file.path(path_to_camels, 'vege.txt'))
    topo <- read_csv2(file.path(path_to_camels, 'topo.txt'))
    geol <- read_csv2(file.path(path_to_camels, 'geol.txt'))
    soil <- read_csv2(file.path(path_to_camels, 'soil.txt'))
    clim <- read_csv2(file.path(path_to_camels, 'clim.txt'))
    hydro <- read_csv2(file.path(path_to_camels, 'hydro.txt'))
    
    col_categories <- c(rep('vege', ncol(vege) - 1),
                        rep('topo', ncol(topo) - 1),
                        rep('geol', ncol(geol) - 1),
                        rep('soil', ncol(soil) - 1),
                        rep('clim', ncol(clim) - 1),
                        rep('hydr', ncol(hydro) - 1))
        
    category_colors <- c('vege' = 'forestgreen',
                         'topo' = 'black',
                         'geol' = 'gray40',
                         'soil' = 'orange4',
                         'clim' = 'darkblue',
                         'hydr' = 'turquoise3')
    
    d <- bind_rows(vege, topo, geol, soil, clim, hydro)
    
    facet_dims <- ceiling(sqrt(length(col_categories)))
    par(mfrow = c(facet_dims, facet_dims))
    
    for(i in 2:ncol(d)){
        
        
    # d <- attr_categories %>% 
    #     map_dfr(function(x) read_csv2(paste0(path_to_camels, '/', x, '.txt'))) %>% 
    #     select(gauge_id, !!varx, !!vary)
    
    # plt <- plot_ly(
    #           x = ~ get(varx),
    #             y = ~ get(y_tvar),
    #             size = ~ get(size_tvar),
    #             color = ~ get(col_by),
    #             colors = safe_cols,
    #             frame = ~Date,
    #             type = "scatter",
    #             mode = "markers",
    #             text = ~ paste0(size_var, " ", size_unit, ":", get(size_var))
    #         ) %>%
    #         plotly::layout(
    #             xaxis = list(title = paste0(x_var, " ", x_unit)),
    #             yaxis = list(title = paste0(y_var, " ", y_unit)),
    #             paper_bgcolor = "rgba(0,0,0,0)",
    #             plot_bgcolor = "rgba(0,0,0,0)"
    #         )

}
```

```{r camels supplement}
ms_download_ws_attr(project_root, dataset = 'CAMELS summaries')
ms_cmls <- ms_load_product(project_root, prodname = 'ws_attr_CAMELS_summaries')

apply(ms_cmls, MARGIN = 2, function(x) round(sum(! is.na(x)) / length(x), 2))
```

```{r camels view 2}

```
