---
title: 'Macroscale watershed science'
subtitle: 'Hacking Limnology 2023'
output:
    html_document:
        theme: 'lumen'
        toc: yes
        toc_float:
            collapsed: true
        toc_depth: 3
---

```{r setup1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = ' ')
```

---

# Before the workshop

Please install the following packages:
```{r install, eval=FALSE}

#macrosheds
devtools::install_github('https://github.com/MacroSHEDS/macrosheds.git')

#tidyverse (dplyr, stringr, readr, etc.)
install.packages('tidyverse')
```

Then download the MacroSheds data we will use
```{r dl, eval=FALSE}

library(macrosheds)

# this is where files will be downloaded. feel free to change it.
project_root <- '~/macrosheds_workshop'

# datasets we will use
ms_download_ws_attr(project_root, dataset = 'summaries')
ms_download_core_data(project_root, domains = c('niwot', 'plum', 'east_river', 'santee'))

```

These packages are optional (only used here and there, and the overall sequence doesn't depend on them)
```{r install optional, eval=FALSE}

install.packages('xts')         # time series representation
install.packages('dygraphs')    # interactive plots
install.packages('factoextra')  # ordination, etc.
install.packages('mapview')     # interactive maps
install.packages('data.table')  # efficient computation
install.packages('whitebox')    # geospatial processing
whitebox::install_whitebox() #additional whitebox binaries. if this fails, use the next line
# whitebox::install_whitebox(platform = 'linux_musl')
```

---

# MacroSheds

_*long-term watershed ecosystem studies, harmonized*_

## Goals
 + enable calculation of input/output solute fluxes from diverse watersheds
 + lower barriers to accessing diverse watershed data through data harmonization and visualization
 + investigate:
   + variation in magnitude, timing, form of N exports
   + broad effects of watershed acidification
   + variation in mineral weathering
   + watershed sensitivity to climate change
 
## Components
 + [macrosheds.org](https://macrosheds.org)
 + [The dataset](https://portal.edirepository.org/nis/mapbrowse?scope=edi&identifier=1262)
 + [R package](https://github.com/MacroSHEDS/macrosheds)
 + [All project code](https://github.com/MacroSHEDS)
 
## Terms
 + site: an individual gauging station or stream sampling location and its watershed
 + domain: one or more sites under common management
 + network: one or more domains under common funding/leadership
 + flux: solute mass normalized to watershed area, over time (kg/ha/d)
 
# Related resources

 + [GEMStat](https://gemstat.org/) - global water quality database
 + [GLORICH](https://www.geo.uni-hamburg.de/en/geologie/forschung/aquatische-geochemie/glorich.html) - global river chemistry database
 + [Water Quality Portal (WQP)](https://www.waterqualitydata.us/) - USA water quality database
 + [Global River Water Quality Archive](https://essd.copernicus.org/articles/13/5483/2021/) - subset of: GEMStat + GLORICH + WQP + CESI (Canada) + Waterbase (Europe)
 + CAMELS - Catchment Attributes and MEteorology for Large-sample Studies
   + [USA](https://gdex.ucar.edu/dataset/camels.html)
   + [Australia](https://www.webofscience.com/wos/woscc/full-record/WOS:000683780500004?SID=USW2EC0D4DYFZoLYt17MtaFngvTOg)
   + [Brazil](https://www.webofscience.com/wos/woscc/full-record/WOS:000569420300002?SID=USW2EC0D4DYFZoLYt17MtaFngvTOg)
   + [Chile](https://www.webofscience.com/wos/woscc/full-record/WOS:000449995800002?SID=USW2EC0D4DYFZoLYt17MtaFngvTOg)
   + [Great Britain](https://www.webofscience.com/wos/woscc/full-record/WOS:000580860000002?SID=USW2EC0D4DYFZoLYt17MtaFngvTOg)
 + [Caravan](https://zenodo.org/record/7944025) - subset of: CAMELS + HYSETS (N. Am.) + LamaH-CE (central Europe)
+ [CAMELS-Chem](https://hess.copernicus.org/preprints/hess-2022-81/) - chemistry for CAMELS-US

# Part 1: Setup and overview

```{r setup2}

suppressPackageStartupMessages(library(tidyverse))
library(macrosheds)

options(readr.show_progress = FALSE,   #prevent gratuitous printing from read_csv
        readr.show_col_types = FALSE,  #same
        timeout = 600)                 #allow more time for dataset downloads

project_root <- '~/macrosheds_workshop'
```

## Retrieval of basic site data, watershed summary attributes

```{r retrieve1, cache=TRUE}

ms_sites <- ms_load_sites() %>% 
    filter(site_type == 'stream_gauge') #some precip gauges have the same names as stream gauges!

?ms_download_ws_attr
# ms_download_ws_attr(project_root, dataset = 'summaries')

?ms_load_product
ws_smry <- ms_load_product(project_root, prodname = 'ws_attr_summaries')
```

 + watershed summary attribute definitions: [https://portal.edirepository.org/nis/metadataviewer?packageid=edi.1262.1] -> Data Entities -> `ws_attr_summaries.csv`
 + two-letter variable prefixes: `variable_category_codes_ws_attr.csv`, `variable_data_source_codes_ws_attr.csv`

## Principal component analysis of watershed summary attributes

```{r pca}

suppressPackageStartupMessages(library(factoextra))

pca_data <- ws_smry %>% 
    select(where( ~sum(is.na(.)) / length(.) < 0.3)) %>% #drop columns with >= 30% missing values
    drop_na()                                            #drop rows with any missing values

domains <- pca_data$domain

pca_data <- pca_data %>% 
    select(-site_code, -domain, -network) %>% 
    select(where(~sd(., na.rm = TRUE) != 0)) %>%         #drop columns with no variance
    as.matrix()

smry_categories <- substr(colnames(pca_data), 1, 1)
category_map <- c('c' = 'climate', 'h' = 'hydrology', 'l' = 'landcover',
                  'p' = 'parentmat', 't' = 'terrain', 'v' = 'vegetation',
                  'w' = 'ws area')
smry_categories <- factor(category_map[match(smry_categories, names(category_map))])

pca <- prcomp(pca_data, center = TRUE, scale. = TRUE)

fviz_eig(pca)

fviz_pca_biplot(pca, geom.var = 'arrow', geom.ind = 'point', title = '',
                col.var = smry_categories, palette = 'Dark2')

fviz_pca_biplot(pca, geom.var = '', geom.ind = 'point', title = '',
                col.ind = as.factor(domains))
```

## Identify domains at elevational extremes (2 of each)

```{r identify target domains}

elev_extreme_domains <- ws_smry %>% 
    filter(! is.na(te_elev_mean)) %>% #no watersheds delineated for McMurdo LTER
    group_by(domain) %>% 
    summarize(domain_mean_elev = mean(te_elev_mean)) %>% 
    ungroup() %>% 
    arrange(desc(domain_mean_elev)) %>% 
    slice(c(1:2, (n() - 1):n())) %>% #2 highest and 2 lowest domains, by average watershed elevation
    print() %>%
    pull(domain)
```

## Load site-variable catalog; filter by domain

```{r filter1}

?ms_load_variables
sitevars <- ms_load_variables('timeseries_by_site')

sitevars <- filter(sitevars,
                   domain %in% elev_extreme_domains,
                   chem_category == 'stream_conc')

length(unique(sitevars$site_code)) # n sites
```

## What variables are shared by at least one site from each of these 4 domains?

```{r shared variables}

get_shared_domain_vars <- function(df){

    df %>% 
        group_split(domain) %>% 
        map(~ pluck(.x, 'variable_code')) %>% 
        reduce(intersect)
}

get_shared_domain_vars(sitevars)
```

## Again, but filter for variables recorded for >= 2 years, at >= average rate of 36 obs/year

```{r shared variables filt}

sitevars <- sitevars %>% 
    mutate(ndays = difftime(last_record_utc, first_record_utc, unit = 'days')) %>% 
    filter(ndays >= 2 * 365.25,
           mean_obs_per_day * 365.25 >= 36)

get_shared_domain_vars(sitevars)
```

## Retrieve core time-series data

```{r retrieve2, cache=TRUE}

?ms_download_core_data
# ms_download_core_data(project_root, domains = elev_extreme_domains)

?ms_load_product
(doc <- ms_load_product(
    project_root,
    prodname = 'stream_chemistry',
    filter_vars = 'DOC',
    domain = elev_extreme_domains,
    warn = FALSE
))

unique(doc$var)
table(doc$ms_status)
table(doc$ms_interp)
```

 + time-series column definitions: [https://portal.edirepository.org/nis/metadataviewer?packageid=edi.1262.1] -> Data Entities -> e.g. `timeseries_boulder.csv`
 + two-letter variable_code prefixes (ts only)
   + G = grab sample
   + I = installed instrument
   + S = sensor
   + N = non-sensor
 + variable units and full names are included in `sitevars`

# Part 2: Which variables best predict DOC at high and low elevation sites?

## remove questionable observations; plot time series

```{r ms_status}

doc_wide <- doc %>% 
    filter(ms_status == 0) %>% #remove ms_status == 1 (questionable)
    select(datetime, site_code, val) %>% 
    left_join(select(ms_sites, site_code, domain), #join column of domain names
              by = 'site_code') %>% 
    filter(! is.na(domain)) %>%  #some site data is missing in MS version 1 (whoops)
    pivot_wider(names_from = c(domain, site_code),
                values_from = val,
                names_sep = '__') %>% #column names are of the form <domain>__<site_code>
    arrange(datetime) %>% #make sure it's all sorted chronologically
    complete(datetime = seq(first(datetime), last(datetime), by = 'day')) #explicate missing observations

suppressPackageStartupMessages(library(xts))
library(dygraphs)

dmn_colors <- factor(str_split_fixed(colnames(doc_wide)[-1], '__', n = 2)[, 1])
levels(dmn_colors) <- hcl.colors(length(elev_extreme_domains))

dg <- dygraph(xts(x = doc_wide[, -1], order.by = doc_wide$datetime)) %>% 
    dyRangeSelector()
for(i in 2:ncol(doc_wide)){
    dg <- dySeries(dg, colnames(doc_wide)[i], color = as.character(dmn_colors[i - 1]))
}
dg
```

## Which watershed attributes predict temporal variation in DOC concentration

```{r regression}
#compute CV for each site
#join ws attrs
#glmnet
```

## Convert nitrate-N concentration to daily flux (basic mode)

```{r flux conversion}
NO3_N_conc <- ms_load_product(
    project_root,
    prodname = 'stream_chemistry',
    filter_vars = 'NO3_N',
    domain = 'niwot',
    warn = FALSE
)

Q <- ms_load_product(
    project_root,
    prodname = 'discharge',
    domain = 'niwot',
    warn = FALSE
)

NO3_N_conc <- semi_join(NO3_N_conc, Q, by = 'site_code')

#before
filter(NO3_N_conc, datetime == as.POSIXct('1985-05-09 00:00:00', tz = 'UTC'), site_code == 'ALBION')

NO3_N_flux <- suppressPackageStartupMessages({
    ms_calc_flux(NO3_N_conc, Q, q_type = 'discharge')
})

#after
filter(NO3_N_flux, datetime == as.POSIXct('1985-05-09 00:00:00', tz = 'UTC'), site_code == 'ALBION')
```

## Compute annual nitrate-N load (still in development)

```{r load calc, eval=FALSE}

# this function will be officially included in the macrosheds package when we
# release version 2 of the MacroSheds dataset, which will include monthly and
# yearly load estimates based on Gubbins et al. in prep. For now, you can call it
# with `:::`, which accesses "internal" package functions.
NO3_N_load <- macrosheds:::ms_calc_flux_rsfme(
    NO3_N_conc, Q,
    method = 'beale',
    aggregation = 'annual'
)
```

## WRTDS flux via EGRET package

```{r wrtds, eval=FALSE}

ms_run_egret(stream_chemistry = filter(NO3_N_conc, site_code == 'ALBION'),
             discharge = filter(Q, site_code == 'ALBION'))
```

## (Un)scale flux by watershed area

```{r scale by watershed area}

(kg_ha_d <- slice(NO3_N_flux, 1:3))
(kg_d <- ms_undo_scale_flux_by_area(kg_ha_d))
(kg_ha_d <- ms_scale_flux_by_area(kg_d))
#NOTE: not every function set up to perform error computations yet
```

# Part 3: Useful tools

## Automated citation/acknowledgement for any subset of the MacroSheds dataset

```{r attribution, cache=TRUE}

?ms_generate_attribution

attrib <- ms_generate_attribution(doc, chem_source = 'stream')

ls.str(attrib)
attrib$intellectual_rights_notifications
attrib$intellectual_rights_explanations
t(attrib$full_details_timeseries[1, ])

attrib <- ms_generate_attribution(doc, chem_source = 'stream', write_to_dir = '~')
read_file('~/macrosheds_attribution_information/ms_bibliography.bib') %>% 
    substr(1, 1092) %>% 
    cat()
```

## Watershed delineation

(interactive, so output not included in the HTML version of this document)

*can work where StreamStats fails, especially in small watersheds*

```{r watershed delineation, eval=FALSE}

# whitebox::install_whitebox() #if this fails, use the next line
# whitebox::install_whitebox(platform = 'linux_musl')

tmp <- tempdir()

out <- ms_delineate_watershed(
    lat = 44.21013,
    long = -122.2571,
    crs = 4326,
    write_dir = tmp,
    write_name = 'example_site',
)

select <- dplyr::select #resolve namespace conflict introduced by raster package

str(out) #specifications of your successful delineation
```

## Load spatial data

(watershed boundaries, precip gauge locations, stream gauge locations)

```{r spatial data}

suppressPackageStartupMessages(library(mapview))

ws_bound <- ms_load_spatial_product(project_root, 'ws_boundary', domain = 'boulder')
prcp_gauges <- ms_load_spatial_product(project_root, 'stream_gauge_locations', domain = 'boulder')
strm_gauges <- ms_load_spatial_product(project_root, 'precip_gauge_locations', domain = 'boulder')

mapview(ws_bound) + mapview(prcp_gauges) + mapview(strm_gauges, col.regions = rainbow(n = 3))
```

## More vignettes

Vignettes will only load if you installed `macrosheds` with `build_vignettes=TRUE`, but they're also hosted as markdown files [here](https://github.com/MacroSHEDS/macrosheds/tree/master/vignettes). These provide tutorials on data retrieval, flux calculation, watershed delineation, and precip interpolation.

```{r more vignettes, eval=FALSE}
vignette(package = 'macrosheds')
vignette('ms_watershed_delineation', package = 'macrosheds')
```

```{r other tools}

?ms_synchronize_timestep  # upsample or downsample macrosheds data
?ms_calc_watershed_precip # spatial interpolation of precip gauge data
?ms_separate_baseflow     # baseflow/stormflow separation via hydrostats
```

# Part 4: linking to the CAMELS datasets

## Download CAMELS-US
*(just watershed descriptors and streamflow time series)*

```{r camels}

camels_dir <- file.path(project_root, 'camels')
dir.create(camels_dir, showWarnings = FALSE)

attr_categories <- c('clim', 'geol', 'soil', 'topo', 'vege', 'hydro')

#watershed attributes
for(x in attr_categories){
    download.file(paste0('https://gdex.ucar.edu/dataset/camels/file/camels_', x, '.txt'),
                  destfile = paste0(camels_dir, '/', x, '.txt'))
}

# #Daymet forcings
# download.file('https://gdex.ucar.edu/dataset/camels/file/basin_timeseries_v1p2_modelOutput_daymet.zip',
#               destfile = file.path(camels_dir, 'daymet.zip'))

#observed flow
# download.file('https://gdex.ucar.edu/dataset/camels/file/basin_timeseries_v1p2_metForcing_obsFlow.zip',
#               destfile = file.path(camels_dir, 'flow.zip'))
# unzip(file.path(camels_dir, 'flow.zip')) #untested. be aware of the truncation issue described in ?unzip
```

## Visualize CAMELS

```{r camels view, fig.width = 14, fig.height = 14}

view_attr_dists <- function(path_to_camels){

    char_cols <- c('dom_land_cover', 'geol_1st_class', 'geol_2nd_class',
                   'high_prec_timing', 'low_prec_timing')
    
    vege <- read_delim(file.path(path_to_camels, 'vege.txt'), delim = ';')
    topo <- read_delim(file.path(path_to_camels, 'topo.txt'), delim = ';')
    geol <- read_delim(file.path(path_to_camels, 'geol.txt'), delim = ';')
    soil <- read_delim(file.path(path_to_camels, 'soil.txt'), delim = ';')
    clim <- read_delim(file.path(path_to_camels, 'clim.txt'), delim = ';')
    hydro <- read_delim(file.path(path_to_camels, 'hydro.txt'), delim = ';')

    categories <- c(rep('vege', ncol(vege) - 1),
                    rep('topo', ncol(topo) - 1),
                    rep('geol', ncol(geol) - 1),
                    rep('soil', ncol(soil) - 1),
                    rep('clim', ncol(clim) - 1),
                    rep('hydr', ncol(hydro) - 1))
    
    d <- reduce(list(vege, topo, geol, soil, clim, hydro), full_join, by = 'gauge_id') %>% 
        # select(-matches(!!char_cols)) %>% 
        mutate(across(-c(gauge_id, !!char_cols), as.numeric))
        
    category_colors <- c('vege' = 'forestgreen',
                         'topo' = 'black',
                         'geol' = 'gray50',
                         'soil' = 'orange4',
                         'clim' = 'darkblue',
                         'hydr' = 'turquoise3')
    
    nrows <- ceiling(sqrt(length(categories)))
    par(mfrow = c(nrows, length(categories) / nrows),
        mar = c(1, 2, 0, 0), oma = rep(0.5, 4))
    
    plot(1, 1, type = 'n')
    legend('left', legend = names(category_colors), col = unname(category_colors),
           lty = 1, lwd = 3, bty = 'n')
    legend('right', legend = c('CAMELS', 'MacroSheds'),
           fill = c('gray70', 'darkviolet'), bty = 'n', border = NA)
    
    for(i in 2:ncol(d)){
        
        varname <- colnames(d)[i]
        if(! varname %in% char_cols){
            
            dvar <- d %>% 
                select(gauge_id, !!varname) %>% 
                filter(! is.na(!!varname)) %>% 
                arrange(!!sym(varname))
            
            source_color <- if_else(grepl('^[0-9]{6,}$', dvar$gauge_id), 'gray70', 'darkviolet')
            color <- category_colors[categories[i - 1]]
            # color <- category_colors[names(category_colors) == categories[i - 1]]
            
            barplot(dvar[[varname]], col = source_color, border = source_color)
            box(col = color, lwd = 4)
            mtext(varname, 3, line = -1.5, col = 'black')
        }
    }
}

view_q_dists <- function(path_to_camels){
    
    flowdir <- file.path(path_to_camels, 'basin_dataset_public_v1p2/usgs_streamflow')
    flowfiles <- list.files(flowdir, recursive = TRUE, pattern = 'streamflow_qc\\.txt$',
                            full.names = TRUE)
    
    par(mfrow = c(1, 1), mar = c(2, 4, 1, 1))
    plot(as.Date(c('1930-01-01', '2023-07-30')), log(c(0.001, 1e6)), type = 'n',
         xlab = '', ylab = 'Log discharge (cfs)', yaxt = 'n')
    yaxis <- c(0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 1e5, 1e6)
    axis(2, log(yaxis), labels = as.character(yaxis))
    
    for(f in flowfiles){
        
        flow <- read.fwf(
            f,
            widths = c(9, 4, 3, 3, 9, 2),
            header = FALSE,
            strip.white = TRUE,
            colClasses = 'character',
            col.names = c('gauge_id', 'year', 'month', 'day', 'Q_cfs', 'flag')
        ) %>%
            as_tibble() %>%
            mutate(date = as.Date(paste(year, month, day), format = '%Y %m %d'),
                   Q_cfs = log(as.numeric(Q_cfs))) %>%
            mutate(Q_cfs = if_else(Q_cfs == -999, NA_real_, Q_cfs))
        
        lines(flow$date, flow$Q_cfs, col = adjustcolor('black', alpha.f = 0.3))
        
                   #      mm/d        cfs         m^3/ft^3      mm/m   s/d     m^2
                   # discharge = discharge * 0.028316846 * 1000 * 86400 / ws_area_m2)
    }
}

view_attr_dists(camels_dir)
```

## Merge MacroSheds and CAMELS

```{r camels supplement}

ms_download_ws_attr(project_root, dataset = 'CAMELS summaries')
ms_cmls <- ms_load_product(project_root, prodname = 'ws_attr_CAMELS_summaries') %>% 
    rename(geol_porostiy = geol_porosity, #accommodate typo in CAMELS dataset
           q5 = Q5, q95 = Q95, baseflow_index = baseflow_index_landson) #and some slight name differences

#proportion of missing values per column
apply(ms_cmls, MARGIN = 2, function(x) round(sum(! is.na(x)) / length(x), 2))

camels_ms_dir <- file.path(project_root, 'camels_macrosheds_merge')
dir.create(camels_ms_dir, showWarnings = FALSE)

cmls_topo <- read_delim(file.path(camels_dir, 'topo.txt'), delim = ';') %>% 
    rename(area = area_gages2) %>% #could also use area_geospa_fabric here
    mutate(across(everything(), as.character))
    
ms_cmls %>% 
    select(gauge_id = site_code, matches(colnames(cmls_topo)), 'area') %>%
    mutate(across(everything(), as.character)) %>% 
    bind_rows(cmls_topo) %>% 
    write_delim(file.path(camels_ms_dir, 'topo.txt'), delim = ';')

camels_ms_merge <- function(dataset, read_dir, write_dir){
    
    #dataset: character; one of "vege", "topo", "soil", "geol", "hydro", "clim"
    
    cmls_d <- read_delim(paste0(read_dir, '/', dataset, '.txt'), delim = ';') %>% 
        mutate(across(everything(), as.character))
    
    missing_vars <- setdiff(colnames(cmls_d), colnames(ms_cmls)) %>% 
        str_subset('gauge_id', negate = TRUE)
    
    if(length(missing_vars)){
        print(paste('MacroSheds is missing some CAMELS variables:',
                    paste(missing_vars, collapse = ', ')))
    }
    
    ms_cmls %>% 
        select(gauge_id = site_code, matches(colnames(cmls_d))) %>%
        mutate(across(everything(), as.character)) %>% 
        bind_rows(cmls_d) %>% 
        write_delim(paste0(write_dir, '/', dataset, '.txt'), delim = ';')
}

camels_ms_merge('vege', read_dir = camels_dir, write_dir = camels_ms_dir)
camels_ms_merge('geol', read_dir = camels_dir, write_dir = camels_ms_dir)
camels_ms_merge('soil', read_dir = camels_dir, write_dir = camels_ms_dir)
camels_ms_merge('clim', read_dir = camels_dir, write_dir = camels_ms_dir)
camels_ms_merge('hydro', read_dir = camels_dir, write_dir = camels_ms_dir)
```

## Visualize again

```{r camels view 2, fig.width = 14, fig.height = 14}

view_attr_dists(camels_ms_dir)
```
